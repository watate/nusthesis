\begin{algorithm}
\caption{Simple Velocity Smoother}
\label{main:simple_v_smoother}
\If{Target Velocity < Control Velocity}{Control Velocity = $\min(Target Velocity, Control Velocity + v_c)$}
\ElseIf{Target Velocity > Control Velocity}{Control Velocity = $\max(Target Velocity, Control Velocity - v_c)$}
\Else{Control Velocity = Target Velocity}
\end{algorithm}

Essentially, ~\autoref{main:simple_v_smoother} is an acceleration limiting approach to motion control where a maximum acceleration is assumed. This generates a trapezoidal motion profile similar to ~\autoref{main:fig:motion_profile_1}. 

\subsection{Map-less Navigation Problem}
The map-less navigation problem can be formulated as follows~\cite{xie_learning_2018}:

At time $t \in [0, T]$, the robot takes an action $a_{t} \in A$ according to input $x_{t}$. The input contains a view of the world (e.g. a stack of laser scans, the current speed of the robot, and the target position with respect to the robot's local frame). We assume that the robot can localize itself in the global coordinate frame with a map which enables the calculation of the target position in the local frame. After executing the action, the robot transitions to its next state in the environment and receives a reward $r_{t}$ according to the reward function. The goal of this decision-making process is to reach the maximum discounted accumulative future reward $R_{t} = \sum_{\tau = t}^{T} \gamma^{\tau - t} r_{\tau}$, where $\gamma$ is the discount factor.


\subsection{Smooth Motion Profile}
Consider the two commonly used motion profiles for point-to-point motion:
\begin{figure}[h]
  \centering
  \includegraphics[width=.9\linewidth]{\Pic{png}{motion_profile}}
  \vspace{\BeforeCaptionVSpace}
  \caption{S-curve profile and trapezoidal profile}
  \label{main:fig:motion_profile}
\end{figure}
\newpage

From ~\autoref{main:fig:motion_profile}, it is evident that an S-curve profile is smoother than a trapezoidal profile. However, why do S-curve profiles result in less load oscillation? 


\section{Problem's Relation to Map-less Navigation Problem}
In addition to having a smooth velocity profile, the robot is expected to be able to perform local navigation; that is, the robot is required to avoid obstacles and reach a target position with smooth motion. The local navigation problem can be formulated as follows~\cite{xie_learning_2018}:

At time $t \in [0, T]$, the robot takes an action $a_{t} \in A$ according to input $x_{t}$. The input contains a view of the world (e.g. a stack of laser scans, the current speed of the robot, and the target position with respect to the robot's local frame). We assume that the robot can localize itself in the global coordinate frame with a map which enables the calculation of the target position in the local frame. After executing the action, the robot transitions to its next state in the environment and receives a reward $r_{t}$ according to the reward function. The goal of this decision-making process is to reach the maximum discounted accumulative future reward $R_{t} = \sum_{\tau = t}^{T} \gamma^{\tau - t} r_{\tau}$, where $\gamma$ is the discount factor.

Since the map-less navigation problem also involves its own set of reward features, another contribution of this work is \textbf{to demonstrate the negative impacts to training time that this learning-based approach may involve}.



###########################################################
DDPG interleaves learning an approximator to Q^*(s,a) with learning an approximator to a^*(s), and it does so in a way which is specifically adapted for environments with continuous action spaces. But what does it mean that DDPG is adapted specifically for environments with continuous action spaces? It relates to how we compute the max over actions in \max_a Q^*(s,a).

When there are a finite number of discrete actions, the max poses no problem, because we can just compute the Q-values for each action separately and directly compare them. (This also immediately gives us the action which maximizes the Q-value.) But when the action space is continuous, we can't exhaustively evaluate the space, and solving the optimization problem is highly non-trivial. Using a normal optimization algorithm would make calculating \max_a Q^*(s,a) a painfully expensive subroutine. And since it would need to be run every time the agent wants to take an action in the environment, this is unacceptable.

Because the action space is continuous, the function Q^*(s,a) is presumed to be differentiable with respect to the action argument. This allows us to set up an efficient, gradient-based learning rule for a policy \mu(s) which exploits that fact. Then, instead of running an expensive optimization subroutine each time we wish to compute \max_a Q(s,a), we can approximate it with \max_a Q(s,a) \approx Q(s,\mu(s)). See the Key Equations section details.
Quick Facts

    DDPG is an off-policy algorithm.
    DDPG can only be used for environments with continuous action spaces.
    DDPG can be thought of as being deep Q-learning for continuous action spaces.
    The Spinning Up implementation of DDPG does not support parallelization.

Key Equations

Here, we'll explain the math behind the two parts of DDPG: learning a Q function, and learning a policy.
The Q-Learning Side of DDPG

First, let's recap the Bellman equation describing the optimal action-value function, Q^*(s,a). It's given by

Q^*(s,a) = \underset{s' \sim P}{{\mathrm E}}\left[r(s,a) + \gamma \max_{a'} Q^*(s', a')\right]

where s' \sim P is shorthand for saying that the next state, s', is sampled by the environment from a distribution P(\cdot| s,a).

This Bellman equation is the starting point for learning an approximator to Q^*(s,a). Suppose the approximator is a neural network Q_{\phi}(s,a), with parameters \phi, and that we have collected a set {\mathcal D} of transitions (s,a,r,s',d) (where d indicates whether state s' is terminal). We can set up a mean-squared Bellman error (MSBE) function, which tells us roughly how closely Q_{\phi} comes to satisfying the Bellman equation:

L(\phi, {\mathcal D}) = \underset{(s,a,r,s',d) \sim {\mathcal D}}{{\mathrm E}}\left[
    \Bigg( Q_{\phi}(s,a) - \left(r + \gamma (1 - d) \max_{a'} Q_{\phi}(s',a') \right) \Bigg)^2
    \right]

Here, in evaluating (1-d), we've used a Python convention of evaluating True to 1 and False to zero. Thus, when d==True---which is to say, when s' is a terminal state---the Q-function should show that the agent gets no additional rewards after the current state. (This choice of notation corresponds to what we later implement in code.)

Q-learning algorithms for function approximators, such as DQN (and all its variants) and DDPG, are largely based on minimizing this MSBE loss function. There are two main tricks employed by all of them which are worth describing, and then a specific detail for DDPG.

Trick One: Replay Buffers. All standard algorithms for training a deep neural network to approximate Q^*(s,a) make use of an experience replay buffer. This is the set {\mathcal D} of previous experiences. In order for the algorithm to have stable behavior, the replay buffer should be large enough to contain a wide range of experiences, but it may not always be good to keep everything. If you only use the very-most recent data, you will overfit to that and things will break; if you use too much experience, you may slow down your learning. This may take some tuning to get right.

You Should Know

We've mentioned that DDPG is an off-policy algorithm: this is as good a point as any to highlight why and how. Observe that the replay buffer should contain old experiences, even though they might have been obtained using an outdated policy. Why are we able to use these at all? The reason is that the Bellman equation doesn't care which transition tuples are used, or how the actions were selected, or what happens after a given transition, because the optimal Q-function should satisfy the Bellman equation for all possible transitions. So any transitions that we've ever experienced are fair game when trying to fit a Q-function approximator via MSBE minimization.




Attempt at writing it without referring to any documents:
\section{Deep Learning}
The 'deep' part of deep learning refers to the idea of using multiple layers for a neural network model. In contrast, "shallow" neural networks only have about 2 layers.

How deep learning works is you have a computation graph. The computation graph consists of an input layer, additional layers, as well as an output layer. The input layer is as the name implies, it is the layer where the network receives input and the number of nodes at the input layer is typically defined in terms of the dimensions of the input (e.g. 3-dimensional input would have 3 nodes). The next layers also have nodes where each node is defined as follows: "Nodal value = f(Nodal value from previous layer * weight + bias)" where f is some non-linear function, w is some weight of the connection between the current node and the previous node, and the bias.

The output layer in general also involves some non-linear function. The number of nodes at the output layer also depends on the dimensions of the output.

Weights and biases are updated by optimizing some cost function (non-convex?).

Two operations are involved with deep learning: forward propagation and backward propagation. Forward propagation is the process of generating an output based on an input. An input is fed into the computation graph, and as the values pass through the layers and reaches the final layer an output is generated.

The generated output is compared against a desired output. This is known as supervised learning, where the inputs and outputs of a network are known, and the goal of deep learning is to learn the mapping between those inputs and outputs. The computation graph gets better at producing an output that matches the desired input through backpropagation. Backpropagation begins by calculating the error between the current output and the target (or desired) output. The derivative of this error with respect to the weights and biases of the network is then calculated. The weights and biases are then updated to minimize this error. To explain this, let's think about the derivatives. If the derivative (dError/dWeight) is positive, then a positive change in the weight will result in a positive change in the Error (increase in error). Conversely, if the derivative is negative, then a positive change in the weight will result in a negative change in the Error (decrease in error). Since error is root mean square error (and therefore always positive), a decrease in error will always lead to the minimum value (local or optimal, but never a negative value). The weights are updated based on a learning rate multiplied by the gradient of the error with respect to that weight. 

As this process occurs over time, the network is said to "fit" or "learn" the data.

\section{Reinforcement Learning}

The goal of reinforcement learning is to learn some action, given a state, that maximises the expected reward. The state is represented by the input layer. The input layer captures data about the current state of affairs, and the network goal is to produce an output (which corresponds to action). This works by modelling phenomena as a markov decision process. That is, there is some state transition function that governs the world, and transitions between states occur as a result of actions taken by an agent. This state transition also results in some reward.

