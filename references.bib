
@online{noauthor_how_nodate,
	title = {How Q learning can be used in reinforcement learning},
	url = {https://dataaspirant.com/2018/02/08/q-learning-in-reinforcement-learning/},
	urldate = {2019-10-15},
	file = {How Q learning can be used in reinforcement learning:C\:\\Users\\james\\Google Drive\\Zotero\\storage\\KHU6PK6Q\\q-learning-in-reinforcement-learning.html:text/html}
}

@article{silver_deterministic_nodate,
	title = {Deterministic Policy Gradient Algorithms},
	abstract = {In this paper we consider deterministic policy gradient algorithms for reinforcement learning with continuous actions. The deterministic policy gradient has a particularly appealing form: it is the expected gradient of the action-value function. This simple form means that the deterministic policy gradient can be estimated much more efﬁciently than the usual stochastic policy gradient. To ensure adequate exploration, we introduce an off-policy actor-critic algorithm that learns a deterministic target policy from an exploratory behaviour policy. We demonstrate that deterministic policy gradient algorithms can signiﬁcantly outperform their stochastic counterparts in high-dimensional action spaces.},
	pages = {9},
	author = {Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
	langid = {english},
	file = {Silver et al. - Deterministic Policy Gradient Algorithms.pdf:C\:\\Users\\james\\Google Drive\\Zotero\\storage\\QB6AQ6AL\\Silver et al. - Deterministic Policy Gradient Algorithms.pdf:application/pdf}
}

@article{lillicrap_continuous_2015,
	title = {Continuous control with deep reinforcement learning},
	url = {http://arxiv.org/abs/1509.02971},
	abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to ﬁnd policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies “end-to-end”: directly from raw pixel inputs.},
	journaltitle = {{arXiv}:1509.02971 [cs, stat]},
	author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
	urldate = {2019-10-06},
	date = {2015-09-09},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1509.02971},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Lillicrap et al. - 2015 - Continuous control with deep reinforcement learnin.pdf:C\:\\Users\\james\\Google Drive\\Zotero\\storage\\4JLACK9Y\\Lillicrap et al. - 2015 - Continuous control with deep reinforcement learnin.pdf:application/pdf}
}

@inproceedings{stanley_efficient_2002,
	location = {Honolulu, {HI}, {USA}},
	title = {Efficient evolution of neural network topologies},
	volume = {2},
	isbn = {978-0-7803-7282-5},
	url = {http://ieeexplore.ieee.org/document/1004508/},
	doi = {10.1109/CEC.2002.1004508},
	abstract = {Neuroevolution, i.e. evolving artiﬁcial neural networks with genetic algorithms, has been highly effective in reinforcement learning tasks, particularly those with hidden state information. An important question in neuroevolution is how to gain an advantage from evolving neural network topologies along with weights. We present a method, {NeuroEvolution} of Augmenting Topologies ({NEAT}) that outperforms the best ﬁxed-topology methods on a challenging benchmark reinforcement learning task. We claim that the increased efﬁciency is due to (1) employing a principled method of crossover of different topologies, (2) protecting structural innovation using speciation, and (3) incrementally growing from minimal structure. We test this claim through a series of ablation studies that demonstrate that each component is necessary to the system as a whole and to each other. What results is signiﬁcantly faster learning. {NEAT} is also an important contribution to {GAs} because it shows how it is possible for evolution to both optimize and complexify solutions simultaneously, making it possible to evolve increasingly complex solutions over time, thereby strengthening the analogy with biological evolution.},
	eventtitle = {2002 World Congress on Computational Intelligence - {WCCI}'02},
	pages = {1757--1762},
	booktitle = {Proceedings of the 2002 Congress on Evolutionary Computation. {CEC}'02 (Cat. No.02TH8600)},
	publisher = {{IEEE}},
	author = {Stanley, K.O. and Miikkulainen, R.},
	urldate = {2019-09-07},
	date = {2002},
	langid = {english},
	file = {Stanley and Miikkulainen - 2002 - Efficient evolution of neural network topologies.pdf:C\:\\Users\\james\\Google Drive\\Zotero\\storage\\99J6N63D\\Stanley and Miikkulainen - 2002 - Efficient evolution of neural network topologies.pdf:application/pdf}
}

@article{stern_dissipation_2018,
	title = {Dissipation of stop-and-go waves via control of autonomous vehicles: Field experiments},
	volume = {89},
	issn = {0968090X},
	url = {http://arxiv.org/abs/1705.01693},
	doi = {10.1016/j.trc.2018.02.005},
	shorttitle = {Dissipation of stop-and-go waves via control of autonomous vehicles},
	abstract = {Trafﬁc waves are phenomena that emerge when the vehicular density exceeds a critical threshold. Considering the presence of increasingly automated vehicles in the trafﬁc stream, a number of research activities have focused on the inﬂuence of automated vehicles on the bulk trafﬁc ﬂow. In the present article, we demonstrate experimentally that intelligent control of an autonomous vehicle is able to dampen stop-and-go waves that can arise even in the absence of geometric or lane changing triggers. Precisely, our experiments on a circular track with more than 20 vehicles show that trafﬁc waves emerge consistently, and that they can be dampened by controlling the velocity of a single vehicle in the ﬂow. We compare metrics for velocity, braking events, and fuel economy across experiments. These experimental ﬁndings suggest a paradigm shift in trafﬁc management: ﬂow control will be possible via a few mobile actuators (less than 5\%) long before a majority of vehicles have autonomous capabilities.},
	pages = {205--221},
	journaltitle = {Transportation Research Part C: Emerging Technologies},
	shortjournal = {Transportation Research Part C: Emerging Technologies},
	author = {Stern, Raphael E. and Cui, Shumo and Monache, Maria Laura Delle and Bhadani, Rahul and Bunting, Matt and Churchill, Miles and Hamilton, Nathaniel and Haulcy, R'mani and Pohlmann, Hannah and Wu, Fangyu and Piccoli, Benedetto and Seibold, Benjamin and Sprinkle, Jonathan and Work, Daniel B.},
	urldate = {2019-09-04},
	date = {2018-04},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1705.01693},
	keywords = {Electrical Engineering and Systems Science - Systems and Control},
	file = {Stern et al. - 2018 - Dissipation of stop-and-go waves via control of au.pdf:C\:\\Users\\james\\Google Drive\\Zotero\\storage\\PQWH5489\\Stern et al. - 2018 - Dissipation of stop-and-go waves via control of au.pdf:application/pdf}
}

@article{bojarski_end_2016,
	title = {End to End Learning for Self-Driving Cars},
	url = {http://arxiv.org/abs/1604.07316},
	abstract = {We trained a convolutional neural network ({CNN}) to map raw pixels from a single front-facing camera directly to steering commands. This end-to-end approach proved surprisingly powerful. With minimum training data from humans the system learns to drive in traffic on local roads with or without lane markings and on highways. It also operates in areas with unclear visual guidance such as in parking lots and on unpaved roads. The system automatically learns internal representations of the necessary processing steps such as detecting useful road features with only the human steering angle as the training signal. We never explicitly trained it to detect, for example, the outline of roads. Compared to explicit decomposition of the problem, such as lane marking detection, path planning, and control, our end-to-end system optimizes all processing steps simultaneously. We argue that this will eventually lead to better performance and smaller systems. Better performance will result because the internal components self-optimize to maximize overall system performance, instead of optimizing human-selected intermediate criteria, e.g., lane detection. Such criteria understandably are selected for ease of human interpretation which doesn't automatically guarantee maximum system performance. Smaller networks are possible because the system learns to solve the problem with the minimal number of processing steps. We used an {NVIDIA} {DevBox} and Torch 7 for training and an {NVIDIA} {DRIVE}({TM}) {PX} self-driving car computer also running Torch 7 for determining where to drive. The system operates at 30 frames per second ({FPS}).},
	journaltitle = {{arXiv}:1604.07316 [cs]},
	author = {Bojarski, Mariusz and Del Testa, Davide and Dworakowski, Daniel and Firner, Bernhard and Flepp, Beat and Goyal, Prasoon and Jackel, Lawrence D. and Monfort, Mathew and Muller, Urs and Zhang, Jiakai and Zhang, Xin and Zhao, Jake and Zieba, Karol},
	urldate = {2019-08-31},
	date = {2016-04-25},
	eprinttype = {arxiv},
	eprint = {1604.07316},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv\:1604.07316 PDF:C\:\\Users\\james\\Google Drive\\Zotero\\storage\\2ZPYNIWP\\Bojarski et al. - 2016 - End to End Learning for Self-Driving Cars.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\james\\Google Drive\\Zotero\\storage\\IIQX2S4Q\\1604.html:text/html}
}

@online{noauthor_introduction_nodate,
	title = {Introduction {\textbar} {TensorFlow} Core},
	url = {https://www.tensorflow.org/guide/low_level_intro},
	titleaddon = {{TensorFlow}},
	urldate = {2019-08-26},
	langid = {english},
	file = {Snapshot:C\:\\Users\\james\\Google Drive\\Zotero\\storage\\MFZMXZBK\\low_level_intro.html:text/html}
}

@online{noauthor_how_2018,
	title = {How To Use {SSH} to Connect to a Remote Server in Linux or Windows},
	url = {https://phoenixnap.com/kb/ssh-to-connect-to-remote-server-linux-or-windows},
	abstract = {In this tutorial, you will learn How To Use {SSH} to Connect to a Remote Server in Linux or Windows. Get started with an {SSH} connection to a Server Today!},
	titleaddon = {Knowledge Base by {phoenixNAP}},
	urldate = {2019-08-23},
	date = {2018-09-24},
	langid = {american},
	file = {Snapshot:C\:\\Users\\james\\Google Drive\\Zotero\\storage\\6HNKAHX7\\ssh-to-connect-to-remote-server-linux-or-windows.html:text/html}
}

@article{jordan_machine_2015,
	title = {Machine learning: Trends, perspectives, and prospects},
	volume = {349},
	issn = {0036-8075, 1095-9203},
	url = {http://www.sciencemag.org/cgi/doi/10.1126/science.aaa8415},
	doi = {10.1126/science.aaa8415},
	shorttitle = {Machine learning},
	pages = {255--260},
	number = {6245},
	journaltitle = {Science},
	shortjournal = {Science},
	author = {Jordan, M. I. and Mitchell, T. M.},
	urldate = {2019-08-21},
	date = {2015-07-17},
	langid = {english},
	file = {Jordan and Mitchell - 2015 - Machine learning Trends, perspectives, and prospe.pdf:C\:\\Users\\james\\Google Drive\\Zotero\\storage\\MGI8CX2Z\\Jordan and Mitchell - 2015 - Machine learning Trends, perspectives, and prospe.pdf:application/pdf}
}

@article{lecun_deep_2015,
	title = {Deep learning},
	volume = {521},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/nature14539},
	doi = {10.1038/nature14539},
	pages = {436--444},
	number = {7553},
	journaltitle = {Nature},
	shortjournal = {Nature},
	author = {{LeCun}, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	urldate = {2019-08-21},
	date = {2015-05},
	langid = {english}
}

@online{noauthor_deep_nodate,
	title = {Deep Reinforcement Learning},
	url = {/blog/article/deep-reinforcement-learning},
	abstract = {Humans excel at solving a wide variety of challenging problems, from low-level motor control through to high-level cognitive tasks. Our goal at {DeepMind} is to create artificial agents that can achieve a similar level of performance and generality. Like a human, our agents learn for themselves to achieve successful strategies that lead to the greatest long-term rewards. This paradigm of learning by trial-and-error, solely from rewards or punishments, is known as reinforcement learning ({RL}). Also like a human, our agents construct and learn their own knowledge directly from raw inputs, such as vision, without any hand-engineered features or domain heuristics. This is achieved by deep learning of neural networks. At {DeepMind} we have pioneered the combination of these approaches - deep reinforcement learning - to create the first artificial agents to achieve human-level performance across many challenging domains.Our agents must continually make value judgements so as to select good actions over bad. This knowledge is represented by a Q-network that estimates the total reward that an agent can expect to receive after taking a particular action. Two years ago we introduced the first widely successful algorithm for deep reinforcement learning. The key idea was to use deep neural networks to represent the Q-network, and to train this Q-network to predict total reward.},
	titleaddon = {Deepmind},
	urldate = {2019-08-21},
	file = {Snapshot:C\:\\Users\\james\\Google Drive\\Zotero\\storage\\SHIMWTVK\\deep-reinforcement-learning.html:text/html}
}

@article{li_deep_2017,
	title = {Deep Reinforcement Learning: An Overview},
	url = {http://arxiv.org/abs/1701.07274},
	shorttitle = {Deep Reinforcement Learning},
	abstract = {We give an overview of recent exciting achievements of deep reinforcement learning ({RL}). We discuss six core elements, six important mechanisms, and twelve applications. We start with background of machine learning, deep learning and reinforcement learning. Next we discuss core {RL} elements, including value function, in particular, Deep Q-Network ({DQN}), policy, reward, model, planning, and exploration. After that, we discuss important mechanisms for {RL}, including attention and memory, unsupervised learning, transfer learning, multi-agent {RL}, hierarchical {RL}, and learning to learn. Then we discuss various applications of {RL}, including games, in particular, {AlphaGo}, robotics, natural language processing, including dialogue systems, machine translation, and text generation, computer vision, neural architecture design, business management, finance, healthcare, Industry 4.0, smart grid, intelligent transportation systems, and computer systems. We mention topics not reviewed yet, and list a collection of {RL} resources. After presenting a brief summary, we close with discussions. Please see Deep Reinforcement Learning, {arXiv}:1810.06339, for a significant update.},
	journaltitle = {{arXiv}:1701.07274 [cs]},
	author = {Li, Yuxi},
	urldate = {2019-08-21},
	date = {2017-01-25},
	eprinttype = {arxiv},
	eprint = {1701.07274},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv\:1701.07274 PDF:C\:\\Users\\james\\Google Drive\\Zotero\\storage\\PKABPECU\\Li - 2017 - Deep Reinforcement Learning An Overview.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\james\\Google Drive\\Zotero\\storage\\M8F297HK\\1701.html:text/html}
}

@online{bonner_getting_2019,
	title = {Getting Started With Google Colab},
	url = {https://towardsdatascience.com/getting-started-with-google-colab-f2fff97f594c},
	abstract = {A Basic Tutorial for the Frustrated and Confused},
	titleaddon = {Medium},
	author = {Bonner, Anne},
	urldate = {2019-08-19},
	date = {2019-05-31},
	langid = {english},
	file = {Snapshot:C\:\\Users\\james\\Google Drive\\Zotero\\storage\\EB9LH8PY\\getting-started-with-google-colab-f2fff97f594c.html:text/html}
}

@online{noauthor_build_2018,
	title = {Build your image dataset faster},
	url = {https://www.christianwerner.net/tech/Build-your-image-dataset-faster/},
	abstract = {So you want to build your own image dataset to train a fancy deep learning model? This small tool helps to make generating custom image datasets for building image classifiers less painful.},
	titleaddon = {Christian's Blog},
	urldate = {2019-08-19},
	date = {2018-10-25},
	langid = {english},
	file = {Snapshot:C\:\\Users\\james\\Google Drive\\Zotero\\storage\\4GHAPNHD\\Build-your-image-dataset-faster.html:text/html}
}

@online{noauthor_how_nodate-1,
	title = {How (and why) to create a good validation set · fast.ai},
	url = {https://www.fast.ai/2017/11/13/validation-sets/},
	urldate = {2019-08-19},
	file = {How (and why) to create a good validation set · fast.ai:C\:\\Users\\james\\Google Drive\\Zotero\\storage\\XL252JJG\\validation-sets.html:text/html}
}

@article{buda_systematic_2018,
	title = {A systematic study of the class imbalance problem in convolutional neural networks},
	volume = {106},
	issn = {08936080},
	url = {http://arxiv.org/abs/1710.05381},
	doi = {10.1016/j.neunet.2018.07.011},
	abstract = {In this study, we systematically investigate the impact of class imbalance on classification performance of convolutional neural networks ({CNNs}) and compare frequently used methods to address the issue. Class imbalance is a common problem that has been comprehensively studied in classical machine learning, yet very limited systematic research is available in the context of deep learning. In our study, we use three benchmark datasets of increasing complexity, {MNIST}, {CIFAR}-10 and {ImageNet}, to investigate the effects of imbalance on classification and perform an extensive comparison of several methods to address the issue: oversampling, undersampling, two-phase training, and thresholding that compensates for prior class probabilities. Our main evaluation metric is area under the receiver operating characteristic curve ({ROC} {AUC}) adjusted to multi-class tasks since overall accuracy metric is associated with notable difficulties in the context of imbalanced data. Based on results from our experiments we conclude that (i) the effect of class imbalance on classification performance is detrimental; (ii) the method of addressing class imbalance that emerged as dominant in almost all analyzed scenarios was oversampling; (iii) oversampling should be applied to the level that completely eliminates the imbalance, whereas the optimal undersampling ratio depends on the extent of imbalance; (iv) as opposed to some classical machine learning models, oversampling does not cause overfitting of {CNNs}; (v) thresholding should be applied to compensate for prior class probabilities when overall number of properly classified cases is of interest.},
	pages = {249--259},
	journaltitle = {Neural Networks},
	shortjournal = {Neural Networks},
	author = {Buda, Mateusz and Maki, Atsuto and Mazurowski, Maciej A.},
	urldate = {2019-08-19},
	date = {2018-10},
	eprinttype = {arxiv},
	eprint = {1710.05381},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing, Computer Science - Artificial Intelligence},
	file = {arXiv\:1710.05381 PDF:C\:\\Users\\james\\Google Drive\\Zotero\\storage\\UG27GX46\\Buda et al. - 2018 - A systematic study of the class imbalance problem .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\james\\Google Drive\\Zotero\\storage\\TX8MZPI7\\1710.html:text/html}
}

@online{noauthor_tips_2018,
	title = {Tips for building large image datasets},
	url = {https://forums.fast.ai/t/tips-for-building-large-image-datasets/26688},
	abstract = {Would love to learn different methods people have used to create their own large image training datasets.  I’ll share mine below:  1) Using google-images-download  \$ pip install google\_images\_download  Install Chrome and Chromedriver to download images from command line.  I installed these onto my virtual machine by navigating to the respective download pages on my laptop’s Chrome browser, and then copying/pasting the correct wget command into the virtual terminal using Chrome’s {CurlWget} extensi...},
	titleaddon = {Deep Learning Course Forums},
	urldate = {2019-08-17},
	date = {2018-10-17},
	langid = {english},
	file = {Snapshot:C\:\\Users\\james\\Google Drive\\Zotero\\storage\\5CJVTC78\\26688.html:text/html}
}

@online{tan_parsing_2018,
	title = {Parsing File Names Using Regular Expressions},
	url = {https://medium.com/@jamestjw/parsing-file-names-using-regular-expressions-3e85d64deb69},
	abstract = {I just went through the 1st lecture of fast.ai v3 (2018 Part 1) taught by Jeremy Howard at the Data Institute at {USF}. Halfway through the…},
	titleaddon = {Medium},
	author = {Tan, James},
	urldate = {2019-08-17},
	date = {2018-10-27},
	langid = {english},
	file = {Snapshot:C\:\\Users\\james\\Google Drive\\Zotero\\storage\\9HGJSFWC\\parsing-file-names-using-regular-expressions-3e85d64deb69.html:text/html}
}

@online{noauthor_splunk_nodate,
	title = {Splunk and Tensorflow for Security: Catching the Fraudster with Behavior Biometrics},
	url = {https://www.splunk.com/content/splunk-blogs/en/2017/04/18/deep-learning-with-splunk-and-tensorflow-for-security-catching-the-fraudster-in-neural-networks-with-behavioral-biometrics.html},
	shorttitle = {Splunk and Tensorflow for Security},
	abstract = {Raising the barrier for fraudsters and attackers: how to leverage Splunk and Deep Learning frameworks to discover Behavior Biometrics patterns within user activities},
	titleaddon = {Splunk-Blogs},
	urldate = {2019-08-17},
	langid = {english},
	file = {Snapshot:C\:\\Users\\james\\Google Drive\\Zotero\\storage\\ZXNX6NYN\\deep-learning-with-splunk-and-tensorflow-for-security-catching-the-fraudster-in-neural-networks.html:text/html}
}

@article{zeiler_visualizing_2013,
	title = {Visualizing and Understanding Convolutional Networks},
	url = {http://arxiv.org/abs/1311.2901},
	abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the {ImageNet} benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky {\textbackslash}etal on the {ImageNet} classification benchmark. We show our {ImageNet} model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
	journaltitle = {{arXiv}:1311.2901 [cs]},
	author = {Zeiler, Matthew D. and Fergus, Rob},
	urldate = {2019-08-17},
	date = {2013-11-12},
	eprinttype = {arxiv},
	eprint = {1311.2901},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1311.2901 PDF:C\:\\Users\\james\\Google Drive\\Zotero\\storage\\ZMLTSH42\\Zeiler and Fergus - 2013 - Visualizing and Understanding Convolutional Networ.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\james\\Google Drive\\Zotero\\storage\\JGFAMYY4\\1311.html:text/html}
}

@article{smith_disciplined_2018,
	title = {A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, momentum, and weight decay},
	url = {http://arxiv.org/abs/1803.09820},
	shorttitle = {A disciplined approach to neural network hyper-parameters},
	abstract = {Although deep learning has produced dazzling successes for applications of image, speech, and video processing in the past few years, most trainings are with suboptimal hyper-parameters, requiring unnecessarily long training times. Setting the hyper-parameters remains a black art that requires years of experience to acquire. This report proposes several efﬁcient ways to set the hyper-parameters that signiﬁcantly reduce training time and improves performance. Speciﬁcally, this report shows how to examine the training validation/test loss function for subtle clues of underﬁtting and overﬁtting and suggests guidelines for moving toward the optimal balance point. Then it discusses how to increase/decrease the learning rate/momentum to speed up training. Our experiments show that it is crucial to balance every manner of regularization for each dataset and architecture. Weight decay is used as a sample regularizer to show how its optimal value is tightly coupled with the learning rates and momentum. Files to help replicate the results reported here are available at https://github.com/lnsmith54/{hyperParam}1.},
	journaltitle = {{arXiv}:1803.09820 [cs, stat]},
	author = {Smith, Leslie N.},
	urldate = {2019-08-17},
	date = {2018-03-26},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1803.09820},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
	file = {Smith - 2018 - A disciplined approach to neural network hyper-par.pdf:C\:\\Users\\james\\Google Drive\\Zotero\\storage\\KPTBSY4F\\Smith - 2018 - A disciplined approach to neural network hyper-par.pdf:application/pdf}
}

@article{pfeiffer_reinforced_2018,
	title = {Reinforced Imitation: Sample Efficient Deep Reinforcement Learning for Map-less Navigation by Leveraging Prior Demonstrations},
	url = {http://arxiv.org/abs/1805.07095},
	shorttitle = {Reinforced Imitation},
	abstract = {This work presents a case study of a learning-based approach for target driven map-less navigation. The underlying navigation model is an end-to-end neural network which is trained using a combination of expert demonstrations, imitation learning ({IL}) and reinforcement learning ({RL}). While {RL} and {IL} suffer from a large sample complexity and the distribution mismatch problem, respectively, we show that leveraging prior expert demonstrations for pre-training can reduce the training time to reach at least the same level of performance compared to plain {RL} by a factor of 5. We present a thorough evaluation of different combinations of expert demonstrations, different {RL} algorithms and reward functions, both in simulation and on a real robotic platform. Our results show that the ﬁnal model outperforms both standalone approaches in the amount of successful navigation tasks. In addition, the {RL} reward function can be signiﬁcantly simpliﬁed when using pre-training, e.g. by using a sparse reward only. The learned navigation policy is able to generalize to unseen and real-world environments.},
	journaltitle = {{arXiv}:1805.07095 [cs]},
	author = {Pfeiffer, Mark and Shukla, Samarth and Turchetta, Matteo and Cadena, Cesar and Krause, Andreas and Siegwart, Roland and Nieto, Juan},
	urldate = {2019-12-23},
	date = {2018-08-31},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1805.07095},
	keywords = {Computer Science - Robotics},
	file = {Pfeiffer et al. - 2018 - Reinforced Imitation Sample Efficient Deep Reinfo.pdf:C\:\\Users\\james\\Google Drive\\Zotero\\storage\\29LVDJ2Z\\Pfeiffer et al. - 2018 - Reinforced Imitation Sample Efficient Deep Reinfo.pdf:application/pdf}
}

@article{zhu_safe_2019,
	title = {Safe, Efficient, and Comfortable Velocity Control based on Reinforcement Learning for Autonomous Driving},
	url = {http://arxiv.org/abs/1902.00089},
	abstract = {A model used for velocity control during car following was proposed based on deep reinforcement learning ({RL}). To fulﬁll the dual objectives of imitating human drivers and optimizing driving performance, a reward function was developed by referencing human driving data and combining driving features related to safety, efﬁciency, and comfort. With the reward function, the {RL} agent learns to control vehicle speed in a fashion that maximizes cumulative rewards, through trials and errors in the simulation environment. A total of 1,341 car-following events extracted from the Next Generation Simulation ({NGSIM}) dataset were used to train the model. Car-following behavior produced by the model were compared with that observed in the empirical {NGSIM} data, to demonstrate the model’s ability to follow a lead vehicle safely, efﬁciently, and comfortably. Results show that the model demonstrates the capability of safe, efﬁcient, and comfortable velocity control in that it 1) has small percentages (8\%) of dangerous minimum time to collision values ({\textless} 5s) than human drivers in the {NGSIM} data (35\%); 2) can maintain efﬁcient and safe headways in the range of 1s to 2s; and 3) can follow the lead vehicle comfortably with smooth acceleration. The results indicate that proposed approach could contribute to the development of better autonomous driving systems.},
	journaltitle = {{arXiv}:1902.00089 [cs, stat]},
	author = {Zhu, Meixin and Wang, Yinhai and Pu, Ziyuan and Hu, Jingyun and Wang, Xuesong and Ke, Ruimin},
	urldate = {2019-12-23},
	date = {2019-10-31},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1902.00089},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Robotics},
	file = {Zhu et al. - 2019 - Safe, Efficient, and Comfortable Velocity Control .pdf:C\:\\Users\\james\\Google Drive\\Zotero\\storage\\WHHCBNDN\\Zhu et al. - 2019 - Safe, Efficient, and Comfortable Velocity Control .pdf:application/pdf}
}

@article{zheng_ros_nodate,
	title = {{ROS} Navigation Tuning Guide},
	abstract = {The {ROS} navigation stack is powerful for mobile robots to move from place to place reliably. The job of navigation stack is to produce a safe path for the robot to execute, by processing data from odometry, sensors and environment map. Maximizing the performance of this navigation stack requires some ﬁne tuning of parameters, and this is not as simple as it looks. One who is sophomoric about the concepts and reasoning may try things randomly, and wastes a lot of time.},
	pages = {23},
	author = {Zheng, Kaiyu},
	langid = {english},
	file = {Zheng - ROS Navigation Tuning Guide.pdf:C\:\\Users\\james\\Google Drive\\Zotero\\storage\\IJTJ8CAZ\\Zheng - ROS Navigation Tuning Guide.pdf:application/pdf}
}

@online{lewin_mathematics_nodate,
	title = {Mathematics of Motion Control Profiles},
	url = {https://www.pmdcorp.com/resources/type/articles/get/mathematics-of-motion-control-profiles-article},
	abstract = {To tune motion profiles for maximum performance, understand the mathematics of motion profiles and which profiles are best for your step motor application.},
	author = {Lewin, Chuck},
	urldate = {2019-12-23},
	langid = {english},
	file = {Snapshot:C\:\\Users\\james\\Google Drive\\Zotero\\storage\\N4P7JPBG\\mathematics-of-motion-control-profiles-article.html:text/html}
}

@article{bensaci_nonlinear_2018,
	title = {Nonlinear Control of a differential wheeled mobile robot in real time-Turtlebot 2},
	abstract = {The paper presents a trajectory tracking controller for a non-holonomic differential wheeled mobile robot ({DWMR}) using the robot kinematic model. A description of the robot is presented followed by a hard robot model and a brief presentation of the {ROS} system to control the real robot thereafter. The nonlinear controller that provides stability according to Lyapunov function is formulated followed by a 3D simulation under Gazebo software and a set of experiments was carried out on a real Turtlebot 2 for different types of trajectories. Finally we presented the results which clearly show the effectiveness and the efficiency of our control approach even for complex paths followed by prospects for future work.},
	pages = {8},
	author = {Bensaci, Chaima and Zennir, Youcef and Pomorski, Denis},
	date = {2018},
	langid = {english},
	file = {Bensaci et al. - 2018 - Nonlinear Control of a differential wheeled mobile.pdf:C\:\\Users\\james\\Google Drive\\Zotero\\storage\\PP43PFNH\\Bensaci et al. - 2018 - Nonlinear Control of a differential wheeled mobile.pdf:application/pdf}
}

@misc{noauthor_optimizing_nodate,
	title = {Optimizing Expectations: From Deep Reinforcement Learning to Stochastic Computation Graphs.pdf},
	file = {Optimizing Expectations\: From Deep Reinforcement Learning to Stochastic Computation Graphs.pdf:C\:\\Users\\james\\Google Drive\\Zotero\\storage\\CJEFH23D\\Optimizing Expectations From Deep Reinforcement Learning to Stochastic Computation Graphs.pdf:application/pdf}
}

@book{lavalle_planning_2006,
	location = {Cambridge},
	title = {Planning Algorithms},
	isbn = {978-0-511-54687-7 978-0-521-86205-9},
	url = {https://www.cambridge.org/core/product/identifier/9780511546877/type/book},
	publisher = {Cambridge University Press},
	author = {{LaValle}, Steven M.},
	urldate = {2019-12-29},
	date = {2006},
	langid = {english},
	doi = {10.1017/CBO9780511546877},
	file = {LaValle - 2006 - Planning Algorithms.pdf:C\:\\Users\\james\\Google Drive\\Zotero\\storage\\ENN9WFKL\\LaValle - 2006 - Planning Algorithms.pdf:application/pdf}
}

@article{sutton_policy_nodate,
	title = {Policy Gradient Methods for Reinforcement Learning with Function Approximation},
	abstract = {Function approximation is essential to reinforcement learning, but the standard approach of approximating a value function and determining a policy from it has so far proven theoretically intractable. In this paper we explore an alternative approach in which the policy is explicitly represented by its own function approximator, independent of the value function, and is updated according to the gradient of expected reward with respect to the policy parameters. Williams's {REINFORCE} method and actor-critic methods are examples of this approach. Our main new result is to show that the gradient can be written in a form suitable for estimation from experience aided by an approximate action-value or advantage function. Using this result, we prove for the first time that a version of policy iteration with arbitrary differentiable function approximation is convergent to a locally optimal policy.},
	pages = {7},
	author = {Sutton, Richard S and {McAllester}, David A and Singh, Satinder P and Mansour, Yishay},
	langid = {english},
	file = {Sutton et al. - Policy Gradient Methods for Reinforcement Learning.pdf:C\:\\Users\\james\\Google Drive\\Zotero\\storage\\ECNA6UU9\\Sutton et al. - Policy Gradient Methods for Reinforcement Learning.pdf:application/pdf}
}

@inproceedings{xie_learning_2018,
	location = {Brisbane, {QLD}},
	title = {Learning with Training Wheels: Speeding up Training with a Simple Controller for Deep Reinforcement Learning},
	isbn = {978-1-5386-3081-5},
	url = {https://ieeexplore.ieee.org/document/8461203/},
	doi = {10.1109/ICRA.2018.8461203},
	shorttitle = {Learning with Training Wheels},
	abstract = {Deep Reinforcement Learning ({DRL}) has been applied successfully to many robotic applications. However, the large number of trials needed for training is a key issue. Most of existing techniques developed to improve training efﬁciency (e.g. imitation) target on general tasks rather than being tailored for robot applications, which have their speciﬁc context to beneﬁt from. We propose a novel framework, Assisted Reinforcement Learning, where a classical controller (e.g. a {PID} controller) is used as an alternative, switchable policy to speed up training of {DRL} for local planning and navigation problems. The core idea is that the simple control law allows the robot to rapidly learn sensible primitives, like driving in a straight line, instead of random exploration. As the actor network becomes more advanced, it can then take over to perform more complex actions, like obstacle avoidance. Eventually, the simple controller can be discarded entirely. We show that not only does this technique train faster, it also is less sensitive to the structure of the {DRL} network and consistently outperforms a standard Deep Deterministic Policy Gradient network. We demonstrate the results in both simulation and real-world experiments.},
	eventtitle = {2018 {IEEE} International Conference on Robotics and Automation ({ICRA})},
	pages = {6276--6283},
	booktitle = {2018 {IEEE} International Conference on Robotics and Automation ({ICRA})},
	publisher = {{IEEE}},
	author = {Xie, Linhai and Wang, Sen and Rosa, Stefano and Markham, Andrew and Trigoni, Niki},
	urldate = {2019-12-30},
	date = {2018-05},
	langid = {english},
	file = {Xie et al. - 2018 - Learning with Training Wheels Speeding up Trainin.pdf:C\:\\Users\\james\\Google Drive\\Zotero\\storage\\9HY23UKT\\Xie et al. - 2018 - Learning with Training Wheels Speeding up Trainin.pdf:application/pdf}
}

@article{tai_virtual--real_2017,
	title = {Virtual-to-real Deep Reinforcement Learning: Continuous Control of Mobile Robots for Mapless Navigation},
	url = {http://arxiv.org/abs/1703.00420},
	shorttitle = {Virtual-to-real Deep Reinforcement Learning},
	abstract = {We present a learning-based mapless motion planner by taking the sparse 10-dimensional range ﬁndings and the target position with respect to the mobile robot coordinate frame as input and the continuous steering commands as output. Traditional motion planners for mobile ground robots with a laser range sensor mostly depend on the obstacle map of the navigation environment where both the highly precise laser sensor and the obstacle map building work of the environment are indispensable. We show that, through an asynchronous deep reinforcement learning method, a mapless motion planner can be trained end-to-end without any manually designed features and prior demonstrations. The trained planner can be directly applied in unseen virtual and real environments. The experiments show that the proposed mapless motion planner can navigate the nonholonomic mobile robot to the desired targets without colliding with any obstacles.},
	journaltitle = {{arXiv}:1703.00420 [cs]},
	author = {Tai, Lei and Paolo, Giuseppe and Liu, Ming},
	urldate = {2020-01-01},
	date = {2017-07-21},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1703.00420},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Robotics},
	file = {Tai et al. - 2017 - Virtual-to-real Deep Reinforcement Learning Conti.pdf:C\:\\Users\\james\\Google Drive\\Zotero\\storage\\ESJQKHE6\\Tai et al. - 2017 - Virtual-to-real Deep Reinforcement Learning Conti.pdf:application/pdf}
}

@article{murray_nonholonomic_1993,
	title = {Nonholonomic motion planning: steering using sinusoids},
	volume = {38},
	issn = {00189286},
	url = {http://ieeexplore.ieee.org/document/277235/},
	doi = {10.1109/9.277235},
	shorttitle = {Nonholonomic motion planning},
	pages = {700--716},
	number = {5},
	journaltitle = {{IEEE} Transactions on Automatic Control},
	shortjournal = {{IEEE} Trans. Automat. Contr.},
	author = {Murray, R.M. and Sastry, S.S.},
	urldate = {2020-01-01},
	date = {1993-05},
	langid = {english},
	file = {Murray and Sastry - 1993 - Nonholonomic motion planning steering using sinus.pdf:C\:\\Users\\james\\Google Drive\\Zotero\\storage\\T6U4H8TU\\Murray and Sastry - 1993 - Nonholonomic motion planning steering using sinus.pdf:application/pdf}
}

@article{wang_dueling_2016,
	title = {Dueling Network Architectures for Deep Reinforcement Learning},
	url = {http://arxiv.org/abs/1511.06581},
	abstract = {In recent years there have been many successes of using deep representations in reinforcement learning. Still, many of these applications use conventional architectures, such as convolutional networks, {LSTMs}, or auto-encoders. In this paper, we present a new neural network architecture for model-free reinforcement learning. Our dueling network represents two separate estimators: one for the state value function and one for the state-dependent action advantage function. The main beneﬁt of this factoring is to generalize learning across actions without imposing any change to the underlying reinforcement learning algorithm. Our results show that this architecture leads to better policy evaluation in the presence of many similar-valued actions. Moreover, the dueling architecture enables our {RL} agent to outperform the state-of-the-art on the Atari 2600 domain.},
	journaltitle = {{arXiv}:1511.06581 [cs]},
	author = {Wang, Ziyu and Schaul, Tom and Hessel, Matteo and van Hasselt, Hado and Lanctot, Marc and de Freitas, Nando},
	urldate = {2020-01-02},
	date = {2016-04-05},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1511.06581},
	keywords = {Computer Science - Machine Learning},
	file = {Wang et al. - 2016 - Dueling Network Architectures for Deep Reinforceme.pdf:C\:\\Users\\james\\Google Drive\\Zotero\\storage\\29NN7NW2\\Wang et al. - 2016 - Dueling Network Architectures for Deep Reinforceme.pdf:application/pdf}
}

@article{li_motion_nodate,
	title = {Motion profile planning for reduced jerk and vibration residuals},
	abstract = {Jerk limitation for a motion stage is important to suppress transient vibration and to reduce the settling time. This paper presents a novel motion control method which aims to reduce jerk and transient vibration for linear motion stages. In this approach, the acceleration profile is designed to obtain a smoother movement without any abrupt corner. The velocity and the displacement profiles are calculated by integration. To implement the proposed motion profile on a {DSP}-based motion controller, a table look-up and linear interpolation method is used to relieve the calculation burden to the {DSP} controller in real-time. A voice coil motor driving motion stage with a fixed-point {DSP} controller is used as the testbed, where a point-to-point linear motion task is designed and implemented to test the proposed motion profile. Experimental results have shown that by using the new profile, the motion induced vibration on the stage frame can be greatly reduced.},
	pages = {6},
	author = {Li, H Z and Gong, Z M and Lin, W and Lippa, T},
	langid = {english},
	file = {Li et al. - Motion profile planning for reduced jerk and vibra.pdf:C\:\\Users\\james\\Google Drive\\Zotero\\storage\\4NB7RPPW\\Li et al. - Motion profile planning for reduced jerk and vibra.pdf:application/pdf}
}

@inproceedings{meckl_optimized_1998,
	location = {Philadelphia, {PA}, {USA}},
	title = {Optimized s-curve motion profiles for minimum residual vibration},
	isbn = {978-0-7803-4530-0},
	url = {http://ieeexplore.ieee.org/document/688324/},
	doi = {10.1109/ACC.1998.688324},
	abstract = {A method for developing optimized point-to-point motion profiles to achieve fist motions with minimum vibration is presented. The proposed approach uses the well-known scurve motion profiles, but optimizes the selection of the ramp-up (and ramp-clown) time. The selection of ramp-up time is based on a ii-equency analysis that minimizes the excitation energy of !he input forcing function at the system natural frequency. Simulation results on a lightly-damped system undergoing point-to-point motions demonstrate that the proposed approach decreases residual vibration by almost an order of magnitude over other approaches, even when the actual natural frequency is in error by 10\%.},
	eventtitle = {Proceedings of the 1998 American Control Conference ({ACC})},
	pages = {2627--2631 vol.5},
	booktitle = {Proceedings of the 1998 American Control Conference. {ACC} ({IEEE} Cat. No.98CH36207)},
	publisher = {{IEEE}},
	author = {Meckl, P.H. and Arestides, P.B.},
	urldate = {2020-01-06},
	date = {1998},
	langid = {english},
	file = {Meckl and Arestides - 1998 - Optimized s-curve motion profiles for minimum resi.pdf:C\:\\Users\\james\\Google Drive\\Zotero\\storage\\R5IWCMB6\\Meckl and Arestides - 1998 - Optimized s-curve motion profiles for minimum resi.pdf:application/pdf}
}

@online{noauthor_stage_nodate,
	title = {Stage: The Stage Simulator},
	url = {https://codedocs.xyz/CodeFinder2/Stage/md_README.html},
	urldate = {2020-01-06},
	file = {Stage\: The Stage Simulator:C\:\\Users\\james\\Google Drive\\Zotero\\storage\\GGP52ANA\\md_README.html:text/html}
}

@article{rojo_solving_nodate,
	title = {Solving an Optimization Problem for Product Delivery with Reinforcement Learning and Deep Neural Networks},
	pages = {117},
	author = {Rojo, Daniel Salgado and Bagén, Toni Lozano},
	langid = {english},
	file = {Rojo and Bagén - Solving an Optimization Problem for Product Delive.pdf:C\:\\Users\\james\\Google Drive\\Zotero\\storage\\V77QHFTY\\Rojo and Bagén - Solving an Optimization Problem for Product Delive.pdf:application/pdf}
}

@inproceedings{singh_end--end_2019,
	title = {End-To-End Robotic Reinforcement Learning without Reward Engineering},
	isbn = {978-0-9923747-5-4},
	url = {http://www.roboticsproceedings.org/rss15/p73.pdf},
	doi = {10.15607/RSS.2019.XV.073},
	abstract = {The combination of deep neural network models and reinforcement learning algorithms can make it possible to learn policies for robotic behaviors that directly read in raw sensory inputs, such as camera images, effectively subsuming both estimation and control into one model. However, realworld applications of reinforcement learning must specify the goal of the task by means of a manually programmed reward function, which in practice requires either designing the very same perception pipeline that end-to-end reinforcement learning promises to avoid, or else instrumenting the environment with additional sensors to determine if the task has been performed successfully. In this paper, we propose an approach for removing the need for manual engineering of reward speciﬁcations by enabling a robot to learn from a modest number of examples of successful outcomes, followed by actively solicited queries, where the robot shows the user a state and asks for a label to determine whether that state represents successful completion of the task. While requesting labels for every single state would amount to asking the user to manually provide the reward signal, our method requires labels for only a tiny fraction of the states seen during training, making it an efﬁcient and practical approach for learning skills without manually engineered rewards. We evaluate our method on real-world robotic manipulation tasks where the observations consist of images viewed by the robot’s camera. In our experiments, our method effectively learns to arrange objects, place books, and drape cloth, directly from images and without any manually speciﬁed reward functions, and with only 1-4 hours of interaction with the real world.},
	eventtitle = {Robotics: Science and Systems 2019},
	booktitle = {Robotics: Science and Systems {XV}},
	publisher = {Robotics: Science and Systems Foundation},
	author = {Singh, Avi and Yang, Larry and Finn, Chelsea and Levine, Sergey},
	urldate = {2020-02-22},
	date = {2019-06-22},
	langid = {english},
	file = {Singh et al. - 2019 - End-To-End Robotic Reinforcement Learning without .pdf:C\:\\Users\\james\\Google Drive\\Zotero\\storage\\6L3JS3NY\\Singh et al. - 2019 - End-To-End Robotic Reinforcement Learning without .pdf:application/pdf}
}

@article{tesauro_temporal_nodate,
	title = {Temporal Difference Learning and {TD}-Gammon},
	pages = {16},
	author = {Tesauro, Gerald},
	langid = {english},
	file = {Tesauro - Temporal Difference Learning and TD-Gammon.pdf:C\:\\Users\\james\\Google Drive\\Zotero\\storage\\34HLXXLN\\Tesauro - Temporal Difference Learning and TD-Gammon.pdf:application/pdf}
}

@article{dahl_context-dependent_2012,
	title = {Context-Dependent Pre-Trained Deep Neural Networks for Large-Vocabulary Speech Recognition},
	volume = {20},
	issn = {1558-7916, 1558-7924},
	url = {http://ieeexplore.ieee.org/document/5740583/},
	doi = {10.1109/TASL.2011.2134090},
	abstract = {We propose a novel context-dependent ({CD}) model for large-vocabulary speech recognition ({LVSR}) that leverages recent advances in using deep belief networks for phone recognition. We describe a pre-trained deep neural network hidden Markov model ({DNN}-{HMM}) hybrid architecture that trains the {DNN} to produce a distribution over senones (tied triphone states) as its output. The deep belief network pre-training algorithm is a robust and often helpful way to initialize deep neural networks generatively that can aid in optimization and reduce generalization error. We illustrate the key components of our model, describe the procedure for applying {CD}-{DNN}-{HMMs} to {LVSR}, and analyze the effects of various modeling choices on performance. Experiments on a challenging business search dataset demonstrate that {CD}-{DNN}-{HMMs} can signiﬁcantly outperform the conventional context-dependent Gaussian mixture model ({GMM})-{HMMs}, with an absolute sentence accuracy improvement of 5.8\% and 9.2\% (or relative error reduction of 16.0\% and 23.2\%) over the {CD}-{GMM}-{HMMs} trained using the minimum phone error rate ({MPE}) and maximum-likelihood ({ML}) criteria, respectively.},
	pages = {30--42},
	number = {1},
	journaltitle = {{IEEE} Transactions on Audio, Speech, and Language Processing},
	shortjournal = {{IEEE} Trans. Audio Speech Lang. Process.},
	author = {Dahl, G. E. and {Dong Yu} and {Li Deng} and Acero, A.},
	urldate = {2020-02-22},
	date = {2012-01},
	langid = {english},
	file = {Dahl et al. - 2012 - Context-Dependent Pre-Trained Deep Neural Networks.pdf:C\:\\Users\\james\\Google Drive\\Zotero\\storage\\XNVR4EKC\\Dahl et al. - 2012 - Context-Dependent Pre-Trained Deep Neural Networks.pdf:application/pdf}
}

@article{krizhevsky_imagenet_2017,
	title = {{ImageNet} classification with deep convolutional neural networks},
	volume = {60},
	issn = {00010782},
	url = {http://dl.acm.org/citation.cfm?doid=3098997.3065386},
	doi = {10.1145/3065386},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the {ImageNet} {LSVRC}-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of ﬁve convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a ﬁnal 1000-way softmax. To make training faster, we used non-saturating neurons and a very efﬁcient {GPU} implementation of the convolution operation. To reduce overﬁtting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the {ILSVRC}-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
	pages = {84--90},
	number = {6},
	journaltitle = {Communications of the {ACM}},
	shortjournal = {Commun. {ACM}},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
	urldate = {2020-02-22},
	date = {2017-05-24},
	langid = {english},
	file = {Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional ne.pdf:C\:\\Users\\james\\Google Drive\\Zotero\\storage\\B25R9UNC\\Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional ne.pdf:application/pdf}
}

@article{gerkey_playerstage_nodate,
	title = {The Player/Stage Project: Tools for Multi-Robot and Distributed Sensor Systems},
	abstract = {This paper describes the Player/Stage software tools applied to multi-robot, distributed-robot and sensor network systems. Player is a robot device server that provides network transparent robot control. Player seeks to constrain controller design as little as possible; it is device independent, non-locking and language- and style-neutral. Stage is a lightweight, highly conﬁgurable robot simulator that supports large populations. Player/Stage is a community Free Software project. Current usage of Player and Stage is reviewed, and some interesting research opportunities opened up by this infrastructure are identiﬁed.},
	pages = {7},
	author = {Gerkey, Brian P and Vaughan, Richard T and Howard, Andrew},
	langid = {english},
	file = {Gerkey et al. - The PlayerStage Project Tools for Multi-Robot an.pdf:C\:\\Users\\james\\Google Drive\\Zotero\\storage\\FJ3DR5AV\\Gerkey et al. - The PlayerStage Project Tools for Multi-Robot an.pdf:application/pdf}
}

@article{SpinningUp2018,
    author = {Achiam, Joshua},
    title = {{Spinning Up in Deep Reinforcement Learning}},
    year = {2018}
}
