\SetPicSubDir{ch-Background}

\chapter{Background}
\vspace{2em}

- DRL
- Deep Q networks
- DDPG

\section{Deep Reinforcement Learning}

Deep reinforcement learning (DRL) is the combination of reinforcement learning and deep learning. This idea of using neural networks for reinforcement learning, however, is not new and can be dated all the way back to Teasauro's TD-Gammon~\cite{tesauro_temporal_nodate}. In the early 2010s, however, the field of deep learning began to find groundbreaking success, particularly in speech recognition~\cite{dahl_context-dependent_2012} and computer vision~\cite{krizhevsky_imagenet_2017}. This success, combined with the advances in computing power, allowed the revival in interest of using deep neural networks as universal function approximators for reinforcement learning - leading to deep reinforcement learning.

Deep reinforcement learning is useful when~\cite{SpinningUp2018}:
\begin{itemize}
\item we have a sequential decision-making problem (which we can represent as a Markov Decision Process)
\item we do not know the optimal behaviour (e.g. multi-modal problem)
\item but we can still evaluate whether behaviours are good or bad
\end{itemize}

\subsection{Markov Decision Process}
A Markov Decision Process (MDP) is a mathematical object that describes an agent interacting with a stochastic environment. It is defined by the following components:
\begin{itemize}
\item $S$: \textbf{state space}, a set of states of the environment
\item $A$: \textbf{action space}, a set of actions, which the agent selects from each timestep (can be discrete or continuous)
\item $P(r, s' | s, a)$: \textbf{transition probability distribution}. For each state s and action $a$, $P$ specifies the probability that the environment will emit reward $r$ and transition to state $s'$.
\end{itemize}

\subsection{Policies}
The end goal is to find a policy $\pi$, which gives you a distribution of actions given states. In DRL, we use parameterized policies: policies whose outputs are computable functions that depend on a set of parameters (e.g. the weights and biases of a neural network) which we can adjust to change the behavior via some optimization algorithm~\cite{SpinningUp2018}.

We denote the parameters of such a policy by $\theta$, and then write this as a subscript on the policy symbol to highlight the connection:

\begin{align}
a_{t} \sim \pi_{\theta}(\cdot | s_{t})
\end{align}





Attempt at writing it without referring to any documents:
\section{Deep Learning}
The 'deep' part of deep learning refers to the idea of using multiple layers for a neural network model. In contrast, "shallow" neural networks only have about 2 layers.

How deep learning works is you have a computation graph. The computation graph consists of an input layer, additional layers, as well as an output layer. The input layer is as the name implies, it is the layer where the network receives input and the number of nodes at the input layer is typically defined in terms of the dimensions of the input (e.g. 3-dimensional input would have 3 nodes). The next layers also have nodes where each node is defined as follows: "Nodal value = f(Nodal value from previous layer * weight + bias)" where f is some non-linear function, w is some weight of the connection between the current node and the previous node, and the bias.

The output layer in general also involves some non-linear function. The number of nodes at the output layer also depends on the dimensions of the output.

Weights and biases are updated by optimizing some cost function (non-convex?).

Two operations are involved with deep learning: forward propagation and backward propagation. Forward propagation is the process of generating an output based on an input. An input is fed into the computation graph, and as the values pass through the layers and reaches the final layer an output is generated.

The generated output is compared against a desired output. This is known as supervised learning, where the inputs and outputs of a network are known, and the goal of deep learning is to learn the mapping between those inputs and outputs. The computation graph gets better at producing an output that matches the desired input through backpropagation. Backpropagation begins by calculating the error between the current output and the target (or desired) output. The derivative of this error with respect to the weights and biases of the network is then calculated. The weights and biases are then updated to minimize this error. To explain this, let's think about the derivatives. If the derivative (dError/dWeight) is positive, then a positive change in the weight will result in a positive change in the Error (increase in error). Conversely, if the derivative is negative, then a positive change in the weight will result in a negative change in the Error (decrease in error). Since error is root mean square error (and therefore always positive), a decrease in error will always lead to the minimum value (local or optimal, but never a negative value). The weights are updated based on a learning rate multiplied by the gradient of the error with respect to that weight. 

As this process occurs over time, the network is said to "fit" or "learn" the data.

\section{Reinforcement Learning}

The goal of reinforcement learning is to learn some action, given a state, that maximises the expected reward. The state is represented by the input layer. The input layer captures data about the current state of affairs, and the network goal is to produce an output (which corresponds to action). This works by modelling phenomena as a markov decision process. That is, there is some state transition function that governs the world, and transitions between states occur as a result of actions taken by an agent. This state transition also results in some reward.

