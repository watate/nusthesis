\SetPicSubDir{ch-Main}
\SetExpSubDir{ch-Main}

\chapter{Main Text}
\label{ch:main}
\vspace{2em}

The main contribution of this work is to test a learning-based approach to smoothing the motion profile of a DRL model. We do this in the following sequence:
\begin{enumerate}
\item Introduce smooth motion profiles
\item Present the network architecture used to apply this learning-based approach
\item Discuss the empirical findings of this learning-based approach as well as compare it with a non-learning based approach
\end{enumerate}

\section{Smooth Motion Profile}
In map-less navigation, we deal with point-to-point motion. That is, the goal of the robot is to move from its starting position and reach some target end position while avoiding obstacles. In terms of motion, point-to-point means that from a stop (zero velocity), the load is accelerated to a constant velocity, and then decelerated such that the final acceleration, and velocity, are zero at the moment the load arrives at the programmed destination. However, achieveing fast motions without residual vibration is a challenge that pervades many applications of point-to-point motion~\cite{meckl_optimized_1998}, including deep reinforcement learning for map-less navigation.

To perform point-to-point motion with minimal vibration, previous works have focused on optimization of the motion profile itself~\cite{meckl_optimized_1998}. For example, ~\autoref{main:fig:motion_profile_1} shows a comparison between a typical trapezoidal motion profile and an (improved) S-curve motion profile. 

\begin{figure}[H]
  \centering
  \includegraphics[width=.9\linewidth]{\Pic{png}{motion_profile_1}}
  \vspace{\BeforeCaptionVSpace}
  \caption{Comparison of Trapezoidal and S-Curve Motion Profiles. Image from Meckl and Arestides, 1998~\cite{meckl_optimized_1998}}
  \label{main:fig:motion_profile_1}
\end{figure}

As can be seen from ~\autoref{main:fig:motion_profile_1}, smoothing a motion profile involves jerk reduction (or some manipulation of the jerk profile). Jerk is the time-derivative of acceleration, that is:
\begin{equation}
j(t) = \frac{da(t)}{dt} = \frac{d^2 v(t)}{dt^2}
\end{equation}

\section{Proposed Approach}
\subsection{Three Different Models}
\label{main:sec:3_models}
In this work, we compare the motion profiles of the following three models:
\begin{enumerate}
\item \textbf{Normal Model}: The default end-to-end neural network trained with deep reinforcement learning to perform map-less navigation
\item \textbf{Model with Velocity Smoother}: The same network as Normal Model but trained with a simple velocity smoother
\item \textbf{Model with Jerk-Learning}: A network very similar to Normal Model but with an added reward function that penalizes the robot for the jerk generated. The inputs of this model are also slightly different (for the reward function to work properly).
\end{enumerate}

\subsection{State}
\label{main:sec:state}
\textit{Normal Model} and \textit{Model with Jerk-Learning} have different features included in their inputs (states) but they both have the same output (action).

At a certain time step $t$, the state of the agent (robot) trained with \textit{Normal Model} is described by:
\begin{itemize}
\item a stack of three 270-degree sparse laser scans y ($y_{t}, y_{t-1}, y_{t-2}$)
\item the current linear and angular velocity of the robot ($v_{t}, \omega_{t}$)
\item x and y components of the robot's displacement from its target position ($d_{x,t}, d_{y,t}$)
\end{itemize}

In the \textit{Model with Jerk-Learning}, the acceleration and jerk of the robot are included in the input layer. The idea is to allow the robot to use information about its motion profile to reduce the jerk generated (in combination with the reward function to incentivize this behaviour). The inputs for this model are:
\begin{itemize}
\item a stack of three 270-degree sparse laser scans y ($y_{t}, y_{t-1}, y_{t-2}$)
\item a stack of linear and angular velocities of the robot ($v_{t}, \omega_{t}, v_{t-1}, \omega_{t-1}, v_{t-2}, \omega_{t-2}$)
\item a stack of linear and angular accelerations of the robot ($accel_{t}, \alpha_{t}, accel_{t-1}, \alpha_{t-1}, accel_{t-2}, \alpha_{t-2}$)
\item a stack of linear and angular jerks of the robot ($j_{t}, \zeta_{t}, j_{t-1}, \zeta_{t-1}, j_{t-2}, \zeta_{t-2}$)
\item x and y components of the robot's displacement from its target position ($d_{x,t}, d_{y,t}$)
\end{itemize}

\subsection{Action}
For all 3 models, the output (action) is a linear velocity ($v_t$) and an angular velocity ($\omega_{t}$). However, the action for the \textit{Model with Velocity Smoother} is slightly different as it is constrained by a simple velocity smoother. In this work, we use the following algorithm for simple velocity smoothing:

\begin{algorithm}[H]
\caption{Simple Velocity Smoother}
\label{main:simple_v_smoother}
\If{Target Velocity < Control Velocity}{Control Velocity = $\min$(Target Velocity, Control Velocity + $v_c$)}
\ElseIf{Target Velocity > Control Velocity}{Control Velocity = $\max$(Target Velocity, Control Velocity - $v_c$)}
\Else{Control Velocity = Target Velocity}
\end{algorithm}
Where:
\begin{itemize}
\item $v_c$ is some constant velocity value
\end{itemize}

Essentially, ~\autoref{main:simple_v_smoother} is an acceleration limiting approach to motion control where a maximum acceleration is assumed. This generates a trapezoidal motion profile similar to the one found in ~\autoref{main:fig:motion_profile_1}. 

\subsection{Simulation Setup}
To enable the DRL agent to learn from trial and error, we use a simple robot simulation environment called Stage. Stage provides a virtual world populated by mobile robots and sensors, along with various objects for the robots to sense and manipulate. We use Stage in ROS (Robot Operating System) to train our DRL agent~\cite{gerkey_playerstage_nodate}.

\subsection{Reward Function}
The reward function, $r(s, a)$, serves as a training signal to encourage or discourage behaviours in the context of a desired task~\cite{zhu_safe_2019}. Thus, the features chosen for the reward function must capture the relevant objectives of the task. In this work, the following reward function was used for the \textit{Normal Model}:
\begin{equation}
Reward, r_{t} = 
\begin{cases}
  -5& \text{if robot crashes},\\
  5& \text{if robot reaches the goal},\\
  \gamma{\bigg((d_{t-1} - d_{t})\Delta{t} - C\bigg)}& \text{otherwise}.
  \\
\end{cases}
\end{equation}
Where:
\begin{itemize}
\item $\gamma$ acts as a discount factor (for infinite-horizon discounted return)
\item $d_{t-1}$ and $d_{t}$ indicate the distance between the robot and its target at two consecutive time stamps
\item $\Delta{t}$ represents the time for each step
\item C is a constant used as a time penalty.
\end{itemize}

Note that the policy learnt with this reward function may be \textit{sub}-optimal. This is due to the fact that the agent is driven by something else other than reaching the target in the shortest time possible. Ideally, a sparse reward function would be better as it would only reward the robot when it actually reaches the target; however, a sparse reward function is challenging for random exploration.

For the \textit{Model with Jerk-Learning}, a new feature which penalizes the robot for jerky movements is added to the reward function. We have chosen to use a dense penalty function for the new feature as it would alleviate the challenges of learning the appropriate motion profile to minimize jerk:
\begin{equation}
Reward, r_{t} = 
\begin{cases}
  -5& \text{if robot crashes},\\ 
  5& \text{if robot reaches the goal},\\
  \gamma{\bigg((d_{t-1} - d_{t})\Delta{t} - C\bigg)} -\\
  \frac{\gamma}{25} \Bigg(\bigg(\frac{j_{t}}{\max{j_{t}}}\bigg)^{2} + \bigg(\frac{\zeta_{t}}{\max{\zeta_{t}}}\bigg)^2\Bigg)& \text{otherwise}.
  \\  
\end{cases}
\end{equation}
Where:
\begin{itemize}
\item $j_t$ is linear jerk at time $t$
\item $\max{j_t}$ is the theoretical maximum linear jerk for each state transition
\item $\zeta_{t}$ is angular jerk at time $t$
\item $\max{\zeta_{t}}$ is the theoretical maximum angular jerk for each state transition
\end{itemize}

\subsection{Network Architecture}
The network architecture we use is largely similar to Xie et. al's Assisted DDPG network~\cite{xie_learning_2018}. The difference, however, is that we have discarded the convolutional neural network (CNN) layers and used a sparser laser beam to allow the agent to learn the policy faster. We reiterate: the goal here is not to produce a state-of-the-art model for map-less navigation, but to empirically study a learning-based approach to jerk reduction. ~\autoref{main:fig:network_architecture} shows the network architecture of the \textit{Normal Model}.

\begin{figure}[H]
  \centering
  \includegraphics[width=.9\linewidth]{\Pic{png}{network_architecture}}
  \vspace{\BeforeCaptionVSpace}
  \caption{Network architecture. Network layers are demonstrated by rectangles. Orange arrows indicate the connectivity between network layers and some other components, e.g. input state and the output of the simple controller. Blue arrows indicate the operation of concatenation (i.e. tf.concat). The final action is selected based on the Q-value predicted by the critic-DQN.}
  \label{main:fig:network_architecture}
\end{figure}

The network architecture of ~\autoref{main:fig:network_architecture} consists of two parts:
\begin{enumerate}
\item \textbf{Policy network and assistive controller (red)}: The policy network consists of a fully-connected layers which estimates the optimal linear and angular velocities for the robot based only on the input state (i.e., laser scans, current speed, and target position in the local frame). This is a very important feature of DDPG as sampling actions from a policy is computationally less expensive than exhaustively evaluating as many discrete actions as possible (since our action space is continuous). Note that the activation functions of the outputs of the policy are sigmoid (linear velocity) and tanh (angular velocity), respectively. The assisive controller is an alternative, switchable policy used to speed up training of the model. It is only used to speed up training, and is discarded eventually.
\item \textbf{Augmented critic network (green)}: The critic-DQN is also constructed with fully-connected layers. It has two branches: one is the critic branch where the action is concatenated into the second layer; the other is the DQN branch where (similar to Xie et. al~\cite{xie_learning_2018}) we apply dueling and double network architecture to speed up training and avoid overestimation. Note that there is no nonlinear activation for its output layers.
\end{enumerate}

The network architecture of the \textit{Model with Velocity Smoother} is exactly the same as the \textit{Normal Model} but with a velocity smoother at the end. It is shown in ~\autoref{main:fig:network_architecture_jerk}:

\begin{figure}[H]
  \centering
  \includegraphics[width=.9\linewidth]{\Pic{png}{network_architecture_vsmooth}}
  \vspace{\BeforeCaptionVSpace}
  \caption{Network architecture of Model with Velocity Smoother. Output layer has a velocity smoother.}
  \label{main:fig:network_architecture_vsmooth}
\end{figure}

The network architecture of the \textit{Model with Jerk-Learning} is slightly different in terms of its inputs (as mentioned in ~\autoref{main:sec:state}). It is shown in ~\autoref{main:fig:network_architecture_jerk}:

\begin{figure}[H]
  \centering
  \includegraphics[width=.9\linewidth]{\Pic{png}{network_architecture_jerk}}
  \vspace{\BeforeCaptionVSpace}
  \caption{Network architecture of Model with Jerk-Learning. Input layer now includes stacks of velocities, accelerations, and jerks of the robot.}
  \label{main:fig:network_architecture_jerk}
\end{figure}


\subsection{Training Methodology}
The training procedure of both networks is according to ~\autoref{main:training_procedure} ~\cite{xie_learning_2018}:
\begin{algorithm}[H]
\caption{Assisted DDPG}
\label{main:training_procedure}
Initialize $A(s, a|\theta^A), Q(s, \sigma|\theta^Q),$ and $\pi(s, \theta^\pi)$\;
Initialize target network $\theta^{A'}, \theta^{Q'},$ and $\theta^{\pi'}$\;
Initialize replay buffer $R$ and exploration noise $\epsilon$\;
\While{Episode is not terminal}{
  Reset the environment\;
  Obtain the initial observation\;
  \For{step = 1}{
    Infer switching $[Q_{policy}, Q_P] = Q(s_t, \sigma|\theta^Q)$
    $\sigma_t = \argmax([Q_{policy}, Q_P])$
    \If{$\sigma_t == 1$}{Sample policy action $a_t = \pi(s_t|\theta^\pi) + \epsilon$}
    \Else{Sample action $a_t$ from external controller}
    Execute $a_t$ and obtain $r_t, s_{t+1}$\;
    Store transition ($s_t, a_t, r_t, s_{t+1}, \sigma_t$) in $R$\;
    Sample N transitions ($s_i, a_i, r_i, s_{i+1}, \sigma_i$) in $R$\;
    Optimize critic-DQN by minimizing loss function\;
    Update policy\;
    Update target networks\;
  }}
\end{algorithm}

All 3 models were trained in 2 different Stage simulation worlds~\autoref{appsec:simple_world},~\autoref{appsec:complex_world}. In each simulation world, the networks were trained until the average performance reached a plateau (i.e. when gradient of average performance with respect to time is approximately zero).

% Here, average performance of the network is obtained according to~\autoref{main:eq:average_performance}. To our knowledge, ~\autoref{main:eq:average_performance} has not been explicitly mentioned in the literature; however, our understanding is that this is what is typically meant by average performance in the mapless navigation literature ~\cite{xie_learning_2018}.
% \begin{equation}
% \text{Average Performance} = \underset{Ep \sim (100 Eps)}{\mathrm E}\bigg[\underset{(s, a) \sim Ep}{\mathrm E} \big(\underset{a \sim \pi}{\max} Q^{\pi}(s, a)\big)\bigg]
% \label{main:eq:average_performance}
% \end{equation}

\section{Experiment Results}
In this section, we discuss and compare the results of training the 3 different models. Visualizing these results, however, remains a difficulty. Consider ~\autoref{main:visualization_difficulty_1}:

\begin{figure}[H]
  \centering
  \includegraphics[width=.9\linewidth]{\Pic{png}{paper_Simple World_Performance_Raw_line}}
  \vspace{\BeforeCaptionVSpace}
  \caption{Graph of Performance vs Training Episodes (Raw).}
  \label{main:visualization_difficulty_1}
\end{figure}

It is difficult to see how well the model is performing over training episodes from ~\autoref{main:visualization_difficulty_1} purely based on the raw results. Therefore, we have chosen to use an exponentially weighted moving average (EWMA) to better visualize how the agent's performance is improving over time (as well as other variables which we will look at in this work). EWMA is calculated as follows:
\begin{equation}
y(t) = \frac{x_t + (1-\alpha_{EWMA})x_{t-1} + (1-\alpha_{EWMA})^{2}x_{t-2} + ... + (1-\alpha_{EWMA})^{t}x_0}{1 + (1-\alpha_{EWMA}) + (1-\alpha_{EWMA}^2) + ... + (1-\alpha_{EWMA})^t}
\end{equation}

Where:
\begin{itemize}
\item $x_t$ is the input
\item $y_t$ is the result
\item $\alpha_{EWMA}$ is the smoothing factor.
\end{itemize}

While it is possible to pass $\alpha_{EWMA}$ directly, it's often easier to think about the span of an EW moment. In this work, unless otherwise specified, we use a span of 1000 for our smoothed results. This is also known as an "N-day EW moving average". That is:
\begin{equation}
\alpha_{EWMA} = \frac{2}{s + 1} \text{ for span s $\geq$ 1}
\end{equation}


Using EWMA, our graph of performance vs training episodes is now smoothened and we are able to see the long-term trends:

\begin{figure}[H]
  \centering
  \includegraphics[width=.9\linewidth]{\Pic{png}{paper_Simple World_Performance_Smooth_line}}
  \vspace{\BeforeCaptionVSpace}
  \caption{Graph of Performance vs Training Episodes (Smooth).}
  \label{main:visualization_difficulty_2}
\end{figure}

% \begin{enumerate}
% \item \textbf{Average Performance over Time}: How fast does the model learn? How well does it perform in the end?
% \item \textbf{Average Jerk over Time}: Does the jerk penalty lead to a smoother motion profile than using a simple velocity smoother?
% \item \textbf{Motion Profile}: What motion profile does the trained model generate? Trapezoidal or S-Curve?
% \end{enumerate}

\subsection{Performance of Models}
\label{main:subsec:performance}
In this section, we compare the performance of the 3 models. We measure performance as the return of a trajectory (which is a measure of cumulative reward along the trajectory). The way this is computed is according to ~\autoref{main:eq:performance}:

\begin{equation}
\label{main:eq:performance}
\text{Performance} = R(\tau) = \sum^{\infty}_{t=0} \gamma^{t} r_t
\end{equation}

~\autoref{main:eq:performance} (infinite-horizon discounted return) is explained in ~\autoref{background:subsec:reward}.

~\autoref{main:performance_1} and ~\autoref{main:performance_2} show that all 3 models display performance improvements at approximately equal rates. All 3 models also plateau at roughly the same performance level. We also observe that the \textit{Model with Jerk-Learning} generally outperforms the \textit{Model with Velocity Smoother}.
\begin{figure}[H]
  \centering
  \includegraphics[width=.8\linewidth]{\Pic{png}{paper_Simple World_Performance_Smooth_line}}
  \vspace{\BeforeCaptionVSpace}
  \caption{Performance vs Training Episodes (Simple World)}
  \label{main:performance_1}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=.8\linewidth]{\Pic{png}{paper_Complex World_Performance_Smooth_line}}
  \vspace{\BeforeCaptionVSpace}
  \caption{Performance vs Training Episodes (Complex World)}
  \label{main:performance_2}
\end{figure}

~\autoref{main:performance_3} and ~\autoref{main:performance_4} show that all 3 models display improvements in success rate at approximately equal rates. All 3 models also end up plateuing at the same success rate. Again, we observe that the \textit{Model with Jerk-Learning} generally outperforms the \textit{Model with Velocity Smoother}.

\begin{figure}[H]
  \centering
  \includegraphics[width=.8\linewidth]{\Pic{png}{paper_Simple World_Success Rate_Smooth_line}}
  \vspace{\BeforeCaptionVSpace}
  \caption{Success Rate vs Training Episodes (Simple World)}
  \label{main:performance_3}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=.8\linewidth]{\Pic{png}{paper_Complex World_Success Rate_Smooth_line}}
  \vspace{\BeforeCaptionVSpace}
  \caption{Success Rate vs Training Episodes (Complex World)}
  \label{main:performance_4}
\end{figure}

\subsection{Motion Profile of Models}
\label{main:subsec:motion}
In this section, we compare the motion profiles of the 3 models. We do this by looking at histograms of linear and angular jerk for each model in each world.

\begin{figure}[H]
  \centering
  \includegraphics[width=.9\linewidth]{\Pic{png}{paper_simple_Linear Jerk_hist}}
  \vspace{\BeforeCaptionVSpace}
  \caption{Histograms of linear jerk for 500 episodes across all 3 models in Simple World ~\autoref{appsec:simple_world}. Model with Jerk-Learning only slightly outperforms the other models here in terms of jerk reduction.}
  \label{main:motion_1}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=.9\linewidth]{\Pic{png}{paper_simple_Angular Jerk_hist}}
  \vspace{\BeforeCaptionVSpace}
  \caption{Histograms of angular jerk for 500 episodes across all 3 models in Simple World ~\autoref{appsec:simple_world}. Model with Velocity Smoother outperforms the other models here in terms of jerk reduction.}
  \label{main:motion_1}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=.9\linewidth]{\Pic{png}{paper_complex_Linear Jerk_hist}}
  \vspace{\BeforeCaptionVSpace}
  \caption{Histograms of linear jerk for 500 episodes across all 3 models in Complex World ~\autoref{appsec:complex_world}. Model with Jerk-Learning only slightly outperforms the other models here in terms of jerk reduction.}
  \label{main:motion_1}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=.9\linewidth]{\Pic{png}{paper_complex_Angular Jerk_hist}}
  \vspace{\BeforeCaptionVSpace}
  \caption{Histograms of angular jerk for 500 episodes across all 3 models in Complex World ~\autoref{appsec:complex_world}. Model with Velocity Smoother outperforms the other models here in terms of jerk reduction.}
  \label{main:motion_1}
\end{figure}

\section{Discussion}
From ~\autoref{main:subsec:performance}, we observe that neither the velocity smoother nor the jerk-learning reward feature seemed to have a significant impact on the training time required.

~\autoref{main:subsec:motion} showed that the \textit{Model with Velocity Smoother} outperformed the other 2 models in terms of jerk reduction. The only cases where it seemed to (very) slightly underperform the \textit{Model with Jerk-Learning} were for linear jerk. We can understand why this is case when we look at the histograms of linear velocities:

\begin{figure}[H]
  \centering
  \includegraphics[width=.9\linewidth]{\Pic{png}{paper_simple_Linear Velocity_hist}}
  \vspace{\BeforeCaptionVSpace}
  \caption{Histograms of linear velocity for 500 episodes across all 3 models in Simple World ~\autoref{appsec:simple_world}}
  \label{main:discussion:linear_velocity:simple}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=.9\linewidth]{\Pic{png}{paper_complex_Linear Velocity_hist}}
  \vspace{\BeforeCaptionVSpace}
  \caption{Histograms of linear velocity for 500 episodes across all 3 models in Complex World ~\autoref{appsec:complex_world}}
  \label{main:discussion:linear_velocity:complex}
\end{figure}

From ~\autoref{main:discussion:linear_velocity:simple} and ~\autoref{main:discussion:linear_velocity:complex} we see that to avoid generating any linear jerk the \textit{Model with Jerk-Learning} only produces the highest linear velocity possible in every state. That is, it avoids changes in velocities to avoid generating jerk. This is also the case for angular velocities as can be seen from ~\autoref{main:discussion:angular_velocity:simple} and ~\autoref{main:discussion:angular_velocity:complex}

\begin{figure}[H]
  \centering
  \includegraphics[width=.9\linewidth]{\Pic{png}{paper_simple_Angular Velocity_hist}}
  \vspace{\BeforeCaptionVSpace}
  \caption{Histograms of angular velocity for 500 episodes across all 3 models in Simple World ~\autoref{appsec:simple_world}}
  \label{main:discussion:angular_velocity:simple}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=.9\linewidth]{\Pic{png}{paper_complex_Angular Velocity_hist}}
  \vspace{\BeforeCaptionVSpace}
  \caption{Histograms of angular velocity for 500 episodes across all 3 models in Complex World ~\autoref{appsec:complex_world}}
  \label{main:discussion:angular_velocity:complex}
\end{figure}

Furthermore, ~\autoref{main:discussion:angular_velocity:simple} and ~\autoref{main:discussion:angular_velocity:complex} also show why the \textit{Model with Velocity Smoother} outperforms the other 2 models in terms of angular jerk: the angular velocities generated by \textit{Model with Velocity Smoother} is more spread out.