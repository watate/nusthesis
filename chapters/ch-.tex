\SetPicSubDir{ch-Main}
\SetExpSubDir{ch-Main}

\chapter{Main Text}
\label{ch:main}
\vspace{2em}

We would like to reiterate that the main contribution of this work \textit{as a case study} is to \textbf{demonstrate a learning-based approach to smoothing the motion profile of a DRL model}. We do this in the following sequence:
\begin{enumerate}
\item Explain what we mean exactly by a smooth motion profile
\item Introduce the network architecture used to demonstrate the effects of this approach
\item Discuss the empirical findings of this learning-based approach as well as comparing it with a non-learning based approach
\end{enumerate}

\section{Theory}
\subsection{Jerk}
As introduced in Chapter~\ref{ch:intro}, a smooth motion profile is taken here to mean a motion profile that minimizes jerk.

Jerk is the time-derivative of acceleration, that is:
\begin{equation}
j(t) = \frac{da(t)}{dt} = \frac{d^2 v(t)}{dt^2}
\end{equation}

To demonstrate that a learning-based approach to jerk reduction can be achieved, we use the network architecture shown in ~\autoref{main:fig:network_architecture}. However, before we reduce jerk, we must first be able to perform map-less navigation.

\subsection{Map-less Navigation Problem}
The map-less navigation problem can be formulated as follows~\cite{xie_learning_2018}:

At time $t \in [0, T]$, the robot takes an action $a_{t} \in A$ according to input $x_{t}$. The input contains a view of the world (e.g. a stack of laser scans, the current speed of the robot, and the target position with respect to the robot's local frame). We assume that the robot can localize itself in the global coordinate frame with a map which enables the calculation of the target position in the local frame. After executing the action, the robot transitions to its next state in the environment and receives a reward $r_{t}$ according to the reward function. The goal of this decision-making process is to reach the maximum discounted accumulative future reward $R_{t} = \sum_{\tau = t}^{T} \gamma^{\tau - t} r_{\tau}$, where $\gamma$ is the discount factor.

\subsection{Network Architecture}
The network architecture we use is largely similar to Xie et. al's AsDDPG network~\cite{xie_learning_2018}. The difference, however, is that we have discarded the convolutional neural network (CNN) layers and used a sparser laser beam to allow the agent to learn the policy faster. We reiterate: the goal here is not to produce a state-of-the-art model for map-less navigation, but to empirically study a learning-based approach to jerk reduction.

\begin{figure}[h]
  \centering
  \includegraphics[width=.9\linewidth]{\Pic{png}{network_architecture}}
  \vspace{\BeforeCaptionVSpace}
  \caption{Network architecture. Network layers are demonstrated by rectangles. Orange arrows indicate the connectivity between network layers and some other components, e.g. input state and the output of the simple controller. Blue arrows indicate the operation of concatenation (i.e. tf.concat). The final action is selected based on the Q-value predicted by the critic-DQN.}
  \label{main:fig:network_architecture}
\end{figure}
\newpage

The network~\ref{main:fig:network_architecture} consists of two parts:
\begin{enumerate}
\item \textbf{Policy network and assistive controller (red)}: The policy network consists of a fully-connected layers which estimates the optimal linear and angular velocities for the robot based only on the input state (i.e., laser scans, current speed, and target position in the local frame). This is a very important feature of DDPG as sampling actions from a policy is computationally less expensive than exhaustively evaluating as many discrete actions as possible (since our action space is continuous). Note that the activation functions of the outputs of the policy are sigmoid (linear velocity) and tanh (angular velocity), respectively. 
\item \textbf{Augmented critic network (green)}: The critic-DQN is also constructed with fully-connected layers. It has two branches: one is the critic branch where the action is concatenated into the second layer; the other is the DQN branch where (similar to Xie et. al~\cite{xie_learning_2018}) we apply dueling and double network architecture to speed up training and avoid overestimation. Note that there is no nonlinear activation for its output layers.
\end{enumerate}







\section{Experiment Work}
\section{Results and Discussion}