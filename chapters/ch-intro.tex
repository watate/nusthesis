\SetPicSubDir{ch-Intro}

\chapter{Introduction}
\vspace{2em}

\section{Brief Description of Project}

This work presents a case study of a learning-based approach to smoothing the movement of a real robotic platform. The robotic platform is operated by an end-to-end neural network which is trained using deep reinforcement learning (DRL) to perform target driven map-less navigation. Previous works~\cite{pfeiffer_reinforced_2018},~\cite{tai_virtual--real_2017},~\cite{xie_learning_2018} have focused more on improving the path-finding or trajectory-generating aspect of map-less navigation while this work focuses on the motion profile planning of map-less navigation. That is, the main idea of this work is to show that smoothing the motion profile generated by the DRL model can be achieved through a learning-based approach. A smooth motion profile is taken here to mean a motion profile that minimizes jerk (time derivative of acceleration).

\section{Problem Definition}
Consider the following histogram of the robotâ€™s angular velocity:

\begin{figure}[h]
  \centering
  \includegraphics[width=.9\linewidth]{\Pic{png}{paper_angular_velocity_hist}}
  \vspace{\BeforeCaptionVSpace}
  \caption{Histogram shows the angular velocity of a well-trained robot in 100 episodes of simulated map-less navigation in Complex World~\ref{appsec:blank_world}}
  \label{intro:fig:paper_angular_velocity_hist}
\end{figure}
\newpage

From Figure ~\ref{intro:fig:paper_angular_velocity_hist}, we observe that after sufficient training, the DRL model only chooses extreme actions for the robot to take (i.e. maximum velocity at 1.0 and -1.0, and minimum velocity at 0). Furthermore in Appendix 4, to perform these actions we observe that the robot has to accelerate instantaneously from 0 to 1.0 and vice versa. In practice, this is not possible for the robot due to acceleration limits; however, conventional DRL models for map-less navigation implicitly assume that arbitrary motion in the configuration space is allowed as long as obstacles are avoided. Moreover, even if this motion profile were possible it would not be ideal as it creates unnecessary jerk and jerk-induced vibrations for the robot. 

Figure ~\ref{intro:fig:paper_angular_jerk_hist} shows that the robot does indeed experience jerk (and at specific magnitudes):

\begin{figure}[h]
  \centering
  \includegraphics[width=.9\linewidth]{\Pic{png}{paper_angular_jerk_hist}}
  \vspace{\BeforeCaptionVSpace}
  \caption{Histogram shows the angular jerk of a well-trained robot in 100 episodes of simulated map-less navigation in Complex World~\ref{appsec:blank_world}}
  \label{intro:fig:paper_angular_jerk_hist}
\end{figure}
\newpage

Motion control methods that achieve fast motions without residual vibrations are already well-known~\cite{meckl_optimized_1998}. Thus, applying an s-curve velocity profile to velocities generated by the DRL model should be able to reduce jerk and vibrations. However, the main contribution of this paper is to show that jerk reduction can be learnt by the DRL model itself. That is, instead of splitting the navigation task into multiple sub-modules (e.g. map-less navigation and motion profile planning), this work represents a case study of an end-to-end approach which uses a direct mapping from sensor data to robot motion commands that achieves both map-less navigation and smooth motion. This is achieved by including jerk as an additional feature in the reward function of the DRL model.

\section{Problem's Relation to Map-less Navigation Problem}
In addition to having a smooth velocity profile, the robot is expected to be able to perform local navigation; that is, the robot is required to avoid obstacles and reach a target position with smooth motion. The local navigation problem can be formulated as follows~\cite{xie_learning_2018}:

At time $t \in [0, T]$, the robot takes an action $a_{t} \in A$ according to input $x_{t}$. The input contains a view of the world (e.g. a stack of laser scans, the current speed of the robot, and the target position with respect to the robot's local frame). We assume that the robot can localize itself in the global coordinate frame with a map which enables the calculation of the target position in the local frame. After executing the action, the robot transitions to its next state in the environment and receives a reward $r_{t}$ according to the reward function. The goal of this decision-making process is to reach the maximum discounted accumulative future reward $R_{t} = \sum_{\tau = t}^{T} \gamma^{\tau - t} r_{\tau}$, where $\gamma$ is the discount factor.

\section{Scope}
As this work is a case-study, the scope of this work is limited to demonstrating the empirical results of applying a learning-based approach to smoothing the movement of a real robotic platform operated by a DRL model. This work does not necessarily show that this approach is desirable in every context; however, it does show the potential improvements to the motion profile (as well as negative impacts to training time) that such a learning-based approach may involve. Thus, as part of the scope, this work is divided into the following sections:

\begin{enumerate}
\item Nothing for now
\end{enumerate}